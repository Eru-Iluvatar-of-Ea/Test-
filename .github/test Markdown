# Comparison of Student Evaluations of Teaching With Online and Paper-Based Administration

**Claudia J. Stanny<sup>1</sup> and James E. Arruda<sup>2</sup>**  
<sup>1</sup> Center for University Teaching, Learning, and Assessment, University of West Florida  
<sup>2</sup> Department of Psychology, University of West Florida  

**Author Note**  
Data collection and preliminary analysis were sponsored by the Office of the Provost and the Student Assessment of Instruction Task Force. Portions of these findings were presented as a poster at the 2016 National Institute on the Teaching of Psychology, St. Pete Beach, Florida, United States. We have no conflicts of interest to disclose.  
Correspondence concerning this article should be addressed to Claudia J. Stanny, Center for University Teaching, Learning, and Assessment, University of West Florida, Building 53, 11000 University Parkway, Pensacola, FL 32514, United States. Email: cstanny@institution.edu

---

## Table of Contents

- [Abstract](#abstract)
- [Introduction](#introduction)
  - [Online Administration of Student Evaluations](#online-administration-of-student-evaluations)
  - [Effects of Format on Response Rates and Student Evaluation Scores](#effects-of-format-on-response-rates-and-student-evaluation-scores)
  - [Purpose of the Present Study](#purpose-of-the-present-study)
- [Method](#method)
  - [Sample](#sample)
  - [Instrument](#instrument)
  - [Design](#design)
- [Results](#results)
  - [Response Rates](#response-rates)
  - [Evaluation Ratings](#evaluation-ratings)
  - [Stability of Ratings](#stability-of-ratings)
- [Discussion](#discussion)
  - [Implications for Practice](#implications-for-practice)
  - [Conclusion](#conclusion)
- [References](#references)

---

## Abstract

When institutions administer **student evaluations of teaching (SETs)** online, *response rates* are lower relative to paper-based administration. We analyzed average SET scores from 364 courses taught during the fall term in 3 consecutive years to determine whether administering SET forms online for all courses in the 3rd year changed the response rate or the average SET score. To control for instructor characteristics, we based the data analysis on courses for which the same instructor taught the course in each of three successive fall terms. Response rates for face-to-face classes declined when SET administration occurred only online. Although average SET scores were reliably lower in Year 3 than in the previous 2 years, the magnitude of this change was minimal (0.11 on a five-item Likert-like scale). We discuss practical implications of these findings for interpretation of SETs and the role of SETs in the evaluation of teaching quality.

**Keywords:** college teaching, student evaluations of teaching, online administration, response rate, assessment

---

## Introduction

Student ratings and evaluations of instruction have a long history as sources of information about teaching quality (Berk, 2013). **Student evaluations of teaching (SETs)** often play a significant role in high-stakes decisions about hiring, promotion, tenure, and teaching awards. As a result, researchers have examined the psychometric properties of SETs and the possible impact of variables such as race, gender, age, course difficulty, and grading practices on average student ratings (Griffin et al., 2014; Nulty, 2008; Spooren et al., 2013). They have also examined how decision makers evaluate SET scores (Boysen, 2015a, 2015b; Boysen et al., 2014; Dewar, 2011). In the last 20 years, considerable attention has been directed toward the consequences of administering SETs online (Morrison, 2011; Stowell et al., 2012) because low *response rates* may have implications for how decision makers should interpret SETs.

### Online Administration of Student Evaluations

Administering SETs online creates multiple benefits. Online administration enables instructors to devote more class time to instruction (vs. administering paper-based forms) and can improve the integrity of the process. Students who are not pressed for time in class are more likely to reflect on their answers and write more detailed comments (Morrison, 2011; Stowell et al., 2012; Venette et al., 2010). Because electronic aggregation of responses bypasses the time-consuming task of transcribing comments (sometimes written in challenging handwriting), instructors can receive summary data and verbatim comments shortly after the close of the term instead of weeks or months into the following term.

Despite these benefits, instructors and students have expressed concerns about online administration of SETs. Students worry that their responses are not confidential when they must use their student identification number to log into the system (Dommeyer et al., 2002). However, breaches of confidentiality can occur even with paper-based administration—e.g., an instructor might recognize student handwriting, or remain present during SET administration (Avery et al., 2006).

### Effects of Format on Response Rates and Student Evaluation Scores

The potential for biased SET findings associated with low *response rates* has been examined in the literature. Contrary to faculty fears that online SETs might be dominated by low-performing students, Avery et al. (2006) found that students with higher grade-point averages (GPAs) were more likely to complete online evaluations. Likewise, Jaquett et al. (2017) reported that students with positive class experiences were more likely to submit evaluations.

Institutions can expect lower *response rates* when SETs are administered online (Avery et al., 2006; Nulty, 2008). However, most researchers have found that mean SET ratings do not change significantly between paper and online formats (Dommeyer et al., 2004; Stowell et al., 2012). Exceptions include Nowell et al. (2010) and Morrison (2011), who reported lower average scores online, with greater variability in individual item responses.

### Purpose of the Present Study

In this study, we examined patterns of responses for online and paper-based SET scores at a midsized U.S. university. We posed two questions:

1. Does the *response rate* or average SET score change when SETs are administered online instead of on paper?
2. What is the minimal *response rate* required to produce stable average SET scores for an instructor?

We analyzed SET data from 364 courses over 3 years, controlling for instructor differences by focusing on courses taught by the same instructor each year.

---

## Method

### Sample

*Response rates* and evaluation ratings were retrieved from archived course evaluation data for 364 courses taught by the same instructor during three consecutive fall terms (2012–2014). The sample included faculty from all five university colleges:

- Social Science and Humanities: 109 (30%)
- Science and Engineering: 82 (23%)
- Education and Professional Studies: 75 (21%)
- Health: 58 (16%)
- Business: 40 (11%)

Approximately 71% were face-to-face courses, and 29% were online, reflecting the university’s course offerings.

### Instrument

The evaluation instrument comprised 18 items, with the first eight measuring instructor quality (e.g., Item 8: “Overall assessment of instructor”) and the rest evaluating course components (e.g., Item 18: “Overall, I would rate the course organization”). Students rated items on a 0–4 scale (*poor* to *excellent*). Item 8 and Item 18 were strongly correlated, *r*(362) = .92, suggesting internal consistency.

### Design

This study leveraged a natural experiment when the university switched to online SETs for all courses in Year 3 (2014). We used a 2 × 3 × 3 factorial design:

- **Course delivery method**: Face-to-face, online
- **Course level**: Beginning undergraduate, advanced undergraduate, graduate
- **Evaluation year**: 2012, 2013, 2014

Dependent measures were *response rate* (percentage of class enrollment) and Item 8 ratings.

---

## Results

### Response Rates

> Response rates for face-to-face classes declined when SET administration occurred only online.

*Response rates* are presented in **Table 1**. Face-to-face courses had higher rates than online courses when administered in-class (Years 1 and 2), but declined in Year 3 (M = 47.18%, SD = 20.11) when all SETs were online, though still slightly higher than online courses (M = 41.60%, SD = 18.23). A significant interaction was found, *F*(1.78, 716) = 101.34, *p* < .001, ηp² = .22.

**Table 1**  
*Means and Standard Deviations for Response Rates (Course Delivery Method by Evaluation Year)*  

| Administration year | Face-to-face course M | Face-to-face course SD | Online course M | Online course SD |
|---------------------|-----------------------|------------------------|-----------------|------------------|
| Year 1: 2012        | 71.72                 | 16.42                  | 32.93           | 15.73            |
| Year 2: 2013        | 72.31                 | 14.93                  | 32.55           | 15.96            |
| Year 3: 2014        | 47.18                 | 20.11                  | 41.60           | 18.23            |

*Note:* SETs were paper-based for face-to-face courses and online for online courses in Years 1 and 2; all were online in Year 3.

### Evaluation Ratings

Year 3 ratings (M = 3.26, SD = 0.60) were lower than Year 1 (M = 3.35, SD = 0.53) and Year 2 (M = 3.38, SD = 0.54), *F*(1.86, 716) = 3.44, *p* = .03, ηp² = .01, though the difference was small (0.11 on a 0–4 scale). Face-to-face courses (M = 3.41, SD = 0.50) rated higher than online courses (M = 3.13, SD = 0.63), *F*(1, 358) = 23.51, *p* = .01, ηp² = .06.

### Stability of Ratings

The correlation between SET scores and *response rates* was small, *r*(362) = .07. Variability above and below a 60% response rate threshold was not significant, *F*(1, 362) = 1.53, *p* = .22.

![Scatterplot depicting the correlation between response rates and evaluation ratings](path/to/figure1.jpg)  
*Figure 1. Scatterplot Depicting the Correlation Between Response Rates and Evaluation Ratings*  
*Note:* Ratings from the 2014 fall term.

---

## Discussion

Online SET administration lowered *response rates*, though online courses saw a 10% increase in Year 3, possibly due to enhanced communication efforts. 

> Although average SET scores were reliably lower in Year 3 than in the previous 2 years, the magnitude of this change was minimal (0.11 on a five-item Likert-like scale).

SET score variability did not significantly decrease with higher *response rates*, challenging the 60%–80% threshold recommendation (Berk, 2012).

### Implications for Practice

#### Improving SET Response Rates

Institutions should:

- Offer incentives
- Ensure high-quality systems
- Promote a culture valuing SET data

#### Evaluating SET Scores

High-stakes decisions require multiple evidence sources beyond SETs, such as portfolios with syllabi and peer observations (Berk, 2013).

### Conclusion

Online SETs offer efficiency but require careful implementation to maintain data usefulness. Multiple measures are essential for evaluating teaching quality.

---

## References

- Avery, R. J., et al. (2006). Electronic course evaluations. *The Journal of Economic Education, 37*(1), 21–37. [https://doi.org/10.3200/JECE.37.1.21-37](https://doi.org/10.3200/JECE.37.1.21-37)
- Berk, R. A. (2013). *Top 10 flashpoints in student ratings and the evaluation of teaching*. Stylus.
- Boysen, G. A. (2015a). Preventing overinterpretation of SETs. *Scholarship of Teaching and Learning in Psychology, 1*(4), 269–282. [https://doi.org/10.1037/stl0000042](https://doi.org/10.1037/stl0000042)
- Dommeyer, C. J., et al. (2004). Gathering faculty teaching evaluations. *Assessment & Evaluation in Higher Education, 29*(5), 611–623. [https://doi.org/10.1080/02602930410001689171](https://doi.org/10.1080/02602930410001689171)
- Nulty, D. D. (2008). Adequacy of response rates. *Assessment & Evaluation in Higher Education, 33*(3), 301–314. [https://doi.org/10.1080/02602930701293231](https://doi.org/10.1080/02602930701293231)
- Stowell, J. R., et al. (2012). Online vs. classroom SETs. *Assessment & Evaluation in Higher Education, 37*(4), 465–473. [https://doi.org/10.1080/02602938.2010.545869](https://doi.org/10.1080/02602938.2010.545869)
- Uttl, B., et al. (2017). Meta-analysis of SETs and learning. *Studies in Educational Evaluation, 54*, 22–42. [https://doi.org/10.1016/j.stueduc.2016.08.007](https://doi.org/10.1016/j.stueduc.2016.08.007)

*(Full list available in original document)*
```
